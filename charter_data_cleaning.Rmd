---
title: "Charter Data Final"
output: html_notebook
---

## Prepare

Init session
```{r echo=F}
rm(list = ls())
library(tidyverse)
library(data.table)
library(stringdist)
library(geosphere)
options(scipen=34)
`%!in%` <- function(a,b) !a %in% b
s <- function() s_df <<- df
l <- function() df <<- s_df
commons <- function(k, n = 20, dec=T)  k %>% table(useNA="a") %>% sort(d=dec) %>% head(n)
# df <- fread("sch_include_02_17_21.csv", integer64="numeric")
# mode <- function(a)  names(sort(table(a), decreasing=T))[1]
# rm(list = ls()[grep("t_.*",ls())])
```

Transform data sources and load into dataset
```{r eval=F}
tictoc::tic()
# Urban Institute ----
# ui
source("src/transform_urban_institute.R")
# it is intentional that NAs are introduced by coercion

# Gradewise Enrollment ----
# gwe
source("src/transform_gradewise_enr.R") 

# Old Dataset ----
df_old <- fread("../Research Question 1/sch_complete_05_26_20.csv", integer64 = "numeric") %>% as.data.table
if ("V1" %in% names(df_old)) { df_old[,V1:=NULL] }

# Create main df
source("src/load_df.R")

# Extract and transform location data
source("src/transform_location.R") 
# s()
# 61586 rows
l()
# Extract and transform school grades & testing data
source("src/transform_grades.R")

rm(df_old,gwe,ui,sg,t_) # Remove source data from workspace
tictoc::toc()

# EXPORT TO CSV ----
# fwrite(df[manual_exclude==F],"sch_include_02_18_21.csv")
# fwrite(dm,"distance_matrix.csv")
# fwrite(ui[!df],"sch_exclude_07_08_20.csv")
# fwrite(df[manual_exclude==T],"sch_exclude_addl_07_08_20.csv")
```

Exponential decay charter penetration (version 2)
```{r}
# init stuff
#dm <- fread("src/distance_matrix.csv",integer64 = "double") %>% as.matrix()
grades <- paste0(c("k",paste0("grade",1:12)),"_enr")

# create a list of enrollments by year. row is location ID, column is grade
y_enr = list()
for (y in 1999:2018) {
    y_enr[[y]] <- (merge(data.table(location_id=1:nrow(dm)),df[year == y, lapply(.SD,function(x) sum(x,na.rm=T)), by=location_id, .SDcols = grades], by="location_id", all.x=T) %>% select(2:14) %>% replace(.,is.na(.),0) %>% as.matrix)
}
# create a list of charter enrollments by year. row is location ID, column is grade
y_cenr = list()
for (y in 1999:2018) {
    y_cenr[[y]] <- (merge(data.table(location_id=1:nrow(dm)),df[year == y & charter=="yes", lapply(.SD,function(x) sum(x,na.rm=T)), by=location_id, .SDcols = grades], by="location_id", all.x=T) %>% select(2:14) %>% replace(.,is.na(.),0) %>% as.matrix)
}

# iterate through hyperparameter r
for (r in c(-1)) { 
  print(r)
  dme <- exp(dm*r) #%>% replace(.,is.infinite(.),0)
  rm("t_lids")
  for (y in 1999:2018) {
      data.table(dme %*% y_enr[[y]]) %>% setnames(.,names(.),paste0("exp_decay_",names(.))) -> t_dm
      data.table(dme %*% y_cenr[[y]]) %>% setnames(.,names(.),paste0("exp_decay_",names(.),"_charter"))-> t_dmc
      t_dm[,`:=`(location_id = 1:nrow(.SD), year=y)]; t_dmc[,`:=`(location_id = 1:nrow(.SD), year=y)]
      if (!exists("t_lids")) t_lids <- merge(t_dm,t_dmc,on=.(location_id,year),all.x=T) 
      else t_lids <- rbind(t_lids, merge(t_dm,t_dmc,on=.(location_id,year),all.x=T))
  }
# merge with df
t_m <- merge(df,t_lids,all.x=T,by=c("location_id","year"))
(rowSums((as.matrix(select(t_m, paste0("exp_decay_",grades,"_charter"))) / as.matrix(select(t_m, paste0("exp_decay_",grades,""))))  *  as.matrix(select(t_m,grades))) / as.matrix(t_m$k12_enrollment)) -> df[[paste0("exp_decay_r",abs(r))]]

#df <- merge(df,t_lids,all.x=T,by=c("location_id","year"))
}
 
 as.matrix(select(t_m, paste0("exp_decay_",grades,"_charter")))
 t_lids %>% group_by(location_id,year) %>% summarize(n=n()) %>% arrange(desc(n))

seq(from=-0.005,to=-.3,by=-0.005) %>% abs %>% gsub("\\.","",.) %>% paste0(collapse = " ")
#rm(y_enr,y_cenr,t_m,dme,t_dm,t_dmc)
```

```{r}

# INIT HYPPERPARAMETERS ----
# max distance where that schools are accounted for, after this their weight is 0
dmax_set = c(1,5,10)
# a correspponds to the curve shape. 1 is straight line from (0,1) to (d,0), a > 1 is concave, a < 1 is convex. 
a_set = c(0.2,1,5)

# INIT OTHER ----
# all grades to iterate through and average
grades <- paste0(c("k",paste0("grade",1:12)),"_enr")

# create a list of enrollments by year. row is location ID, column is grade
y_enr = list()
for (y in 1999:2018) {
    y_enr[[y]] <- (merge(data.table(location_id=1:nrow(dm)),df[year == y, lapply(.SD,function(x) sum(x,na.rm=T)), by=location_id, .SDcols = grades], by="location_id", all.x=T) %>% select(2:14) %>% replace(.,is.na(.),0) %>% as.matrix)
}
# create a list of charter enrollments by year. row is location ID, column is grade
y_cenr = list()
for (y in 1999:2018) {
    y_cenr[[y]] <- (merge(data.table(location_id=1:nrow(dm)),df[year == y & charter=="yes", lapply(.SD,function(x) sum(x,na.rm=T)), by=location_id, .SDcols = grades], by="location_id", all.x=T) %>% select(2:14) %>% replace(.,is.na(.),0) %>% as.matrix)
}

#### LOOP ----
dmax = 3; a = 1.2
(1-(dm[1:10,1:10]/dmax)^a) %>% view
dmf[dmf < 0] <- 0
dmf[1:20,1:20] %>% view
# iterate through hyperparameters
for (dmax in dmax_set) { 
  for (a in a_set) {
  # apply the curving function to the distance matrix
  dmf <- (1-(dm/dmax)^a)
}}

for (r in c(-1)) { 
  print(r)
  dme <- exp(dm*r) #%>% replace(.,is.infinite(.),0)
  rm("t_lids")
  for (y in 1999:2018) {
      data.table(dme %*% y_enr[[y]]) %>% setnames(.,names(.),paste0("exp_decay_",names(.))) -> t_dm
      data.table(dme %*% y_cenr[[y]]) %>% setnames(.,names(.),paste0("exp_decay_",names(.),"_charter"))-> t_dmc
      t_dm[,`:=`(location_id = 1:nrow(.SD), year=y)]; t_dmc[,`:=`(location_id = 1:nrow(.SD), year=y)]
      if (!exists("t_lids")) t_lids <- merge(t_dm,t_dmc,on=.(location_id,year),all.x=T) 
      else t_lids <- rbind(t_lids, merge(t_dm,t_dmc,on=.(location_id,year),all.x=T))
  }
# merge with df
t_m <- merge(df,t_lids,all.x=T,by=c("location_id","year"))
(rowSums((as.matrix(select(t_m, paste0("exp_decay_",grades,"_charter"))) / as.matrix(select(t_m, paste0("exp_decay_",grades,""))))  *  as.matrix(select(t_m,grades))) / as.matrix(t_m$k12_enrollment)) -> df[[paste0("exp_decay_r",abs(r))]]

#df <- merge(df,t_lids,all.x=T,by=c("location_id","year"))
}
 
 as.matrix(select(t_m, paste0("exp_decay_",grades,"_charter")))
 t_lids %>% group_by(location_id,year) %>% summarize(n=n()) %>% arrange(desc(n))

seq(from=-0.005,to=-.3,by=-0.005) %>% abs %>% gsub("\\.","",.) %>% paste0(collapse = " ")
#rm(y_enr,y_cenr,t_m,dme,t_dm,t_dmc)
```


## Analysis 
Summary stats for new variables
```{r}
df <- fread("sch_include_07_18_20.csv", integer64="numeric")

df %>% select(weighted_pct_charter_10m) %>% filter(weighted_pct_charter_10m > 0) %>% skimr::skim()
df %>% select(weighted_pct_charter_5m) %>% filter(weighted_pct_charter_5m > 0) %>% skimr::skim()
df %>% select(weighted_pct_charter_2.5m) %>% filter(weighted_pct_charter_2.5m > 0) %>% skimr::skim()
df %>% select(weighted_pct_charter_1m) %>% filter(weighted_pct_charter_1m > 0) %>% skimr::skim()

df %>% select(comb_ela_proficiency) %>% hist()
df[,comb_ela_proficiency] %>% hist
df[,comb_ela_proficiency] %>% hist
```

```{r}
df$year %>% max
```

```{r}
ns <- sample(df[by=.(ncessch),,.N][N>10,ncessch],500)

df[by=year,,sum(!is.na(comb_math_proficiency))]

df[charter == "no",.(nyears=.N, maxratio1m=max(log(1+weighted_ctr_1m)),maxratio2.5m=max(log(1+weighted_ctr_2.5m)),maxratio5m=max(log(1+weighted_ctr_5m)),maxratio10m=max(log(1+weighted_ctr_10m)),maxfte = max(teachers_fte,na.rm=T)),  by=.(ncessch)][nyears > 5] %>%
  #skimr::skim()
  #ggplot(.) + geom_density(aes(x=c(maxratio1m),color="1m"),alpha=0.3,) + geom_density(aes(x=c(maxratio2.5m),color="2.5m"),alpha=0.3,) + geom_density(aes(x=c(maxratio5m),color="5m"),alpha=0.3,) + geom_density(aes(x=c(maxratio10m),color="10m"),alpha=0.3,)
  ggplot(.,aes(x=year,y=maxfte,fill=maxratio5m>0))

df %>% select(teachers_fte)
df[charter == "no", by=.(year,weighted_ctr_5m>0), .(mean(as.numeric(comb_math_proficiency),na.rm=T), sd(teachers_fte,na.rm=T))][!is.na(weighted_ctr_5m)] %>% ggplot(.,aes(x=year,y=V1,fill=weighted_ctr_5m)) + geom_bar(stat="identity", position="dodge")


df[weighted_ctr_1m %>% is.na]
df[enrollment == prek_enr]
names(df)
```

## Export

```{r}
#        s()          l()

# make sure all names will play nice in stata

# k_enr -> grade0_enr
# setnames(df, names(df)[grepl("exp_decay_k_enr",names(df))], names(df)[grepl("exp_decay_k_enr",names(df))] %>% gsub("k_enr","grade0_enr",.))

# abbreviate pct
setnames(df, names(df), gsub("percente?","pct" ,names(df)))

# replace names
setnames(df, c("english_language_arts_gains_of_lowest_25%", "college_&_career_acceleration(prev_year)", "pct_of_economically_disadvantaged_students", "pct_level_3_and_above_fcat_reading" , "pct_making_learning_gains_in_reading",  "pct_making_learning_gains_in_math", "pct_of_lowest_25p_making_learning_gains_in_reading","k_enr"), 
         c("ela_gains_lowest_25","college_and_career_acc", "pct_econ_disadv_students", "pct_l3_and_above_reading","pct_making_gains_reading",  "pct_making_gains_math", "pct_lowest_25p_gains_reading","grade0_enr"), skip_absent = T)

# drop fully irrelevant columns
(df[,c("gm_street_number","gm_route","gm_postal_code"):=NULL])

# order the columns the way that I like
df %>% select(
  ncessch,year,
  school_name, distinct_charter, distinct_regular_nc,
  k12_enrollment, 
  gm_formatted_address, gm_lat,gm_lon,
  street_location, city_location, zip_location,
  school_type,  charter, 
  c(names(df)[grepl(".*_enr",names(df))]),
  location_verification_method, latlon_verification_method,
  everything()
) -> df

# fwrite(ui[!df],"sch_exclude_07_08_20.csv")
# fwrite(df[manual_exclude==T],"sch_exclude_addl_07_08_20.csv")
fwrite(df[manual_exclude==F],"sch_include_02_17_21.csv")

# validation ----


if (!identical(as.character(), names(df)[grepl("\\.[xy]",names(df))])) warning("There are columns with a .x or .y suffix")
if (!identical(as.character(), names(df)[nchar(names(df)) >= 32])) warning("There are columns with atleast 32 characters")
```


## Data Sources

***Data Sources***

All sources wisll eventually contain data 1999 through 2018, some are currently a year or two behind 2018

*Urban Institute Dataset*
(https://educationdata.urban.org/data-explorer/schools/) downloaded 06/04/20 
Most information comes from the common core of data [partial data dictionary](https://nces.ed.gov/ccd/psadd.asp)
contains general information about:
- enrollment
- lowest/highest grade offered
- charter status
- virtual status
- latitude & longitude
```{r eval=F}
ui %>% sample_n(20)
```

*Latitude & Longitude*
Pulled from one file from ELSI, source of coordinates is CCD
(https://nces.ed.gov/ccd/elsi/) downloaded 06/15/20

*Grade-wise enrollment* 
Pulled as four separate files from ELSI table generator, then combined 
(https://nces.ed.gov/ccd/elsi/) downloaded 06/05/20
```{r eval=F}
gwe %>% select(ncessch, year, everything()) %>% sample_n(10)
```

*School grades datasets*
(http://www.fldoe.org/accountability/accountability-reporting/school-grades/archives.stml)
1999 - 2017
- Test scores
- School grade
```{r eval=F}
sg2014 %>%
  select(key_sch, distnum_schnum,`District Number`,`School Number`,Year,everything()) %>% sample_n(10)

sg2017 %>%
  select(key_sch, distnum_schnum,`District Number`,`School Number`,Year,everything()) %>% sample_n(10)
```

*Student demographics*
(https://nces.ed.gov/ccd/elsi/tableGenerator.aspx)
1997 - 2016
- Total enrollment
- Highest and lowest grade
- Teachers & pupil teacher ratio
```{r eval=F}
dem %>% sample_n(20)
dem$Year %>% table
```

### Misc. notes
1609.34 meters in a mile.

## Looking: 
### Names
Stringdist matrix
```{r}
df[!is.na(street_location) & ncessch %!in% unique(dv$ncessch),.(mdist = stringdist::stringdistmatrix(.SD[,street_location], method="lcs") %>% max(na.rm=T),
      numdist = stringdist::stringdistmatrix(.SD[,gsub("[^0-9d]","",street_location)],method="lcs") %>% max(na.rm=T)),by=ncessch] %>% arrange(mdist) %>% as.data.table() -> t_d

t_d[mdist != -Inf] %>% arrange(desc(mdist + numdist*4))

t_d[mdist != -Inf & (mdist + 4*numdist)<20,ncessch] -> t_d2
df[ncessch %!in% unique(dv$ncessch) & ncessch %in% t_d2] %>% arrange(desc(max_move_dist)) %>% select(max_move_dist,ncessch,year,gm_formatted_address,street_location,city_location,zip_location,gm_lat,gm_lon,school_name,wildcard,location_verification_method, enrollment,charter,ccd_lat,ccd_lon, everything()) %>% arrange(desc(max_move_dist,ncessch,year)) %>% head(2000) %>% View

# unique names of schools that have been open since 2014

df$school_name %>% unique %>% length


str_split(t_names$name,"") %>% unlist %>% table()

#rm(loc_enr,loc_enr2,dm_bool,y,charter_ids,tps_ids,big_enr_charter,big_enr_tps)
```

Googlemaps unique name queries
```{r}
t_names <- fread("src/gm_addresses3.csv") %>% as.data.table(key="name_original")
t_names_merge <- merge(df,t_names,by.x="school_name",by.y="name_original",all.x=T)
t_names_merge[year == 2018 & !is.na(gm_lon.y) & !is.na(gm_lat.y),.(dist = geosphere::distHaversine(c(first(gm_lon.x),first(gm_lat.x)),c(first(gm_lon.y),first(gm_lat.y))), nces = paste0(unique(ncessch),collapse=", ")),by=school_name] %>% arrange(desc(dist))
#t_names_merge[year == 2018 & !is.na(gm_lon.y) & !is.na(gm_lat.y),.(dist = geosphere::distHaversine(c(first(gm_lon.x),first(gm_lat.x)),c(first(gm_lon.y),first(gm_lat.y))))]

t_names_merge[.(paste0(c(gm_lon.x,gm_lat.x),c(gm_lon.y,gm_lat.y)))]
```

```{r}
t_names_merge[ncessch == 120198002022] %>% select(ncessch, year, school_name, gm_lat.x,gm_lon.x,gm_formatted_address,gm_lat.y,gm_lon.y,gm_address)
```

```{r}
t_names_merge[ncessch %in% 120198002022] %>% arrange(desc(max_move_dist)) %>% select(max_move_dist,ncessch,year,gm_formatted_address,street_location,city_location,zip_location,gm_lon.x,gm_lat.x,gm_lon.y,gm_lat.y,school_name,wildcard,location_verification_method, enrollment,charter,ccd_lat,ccd_lon, everything()) %>% arrange(desc(max_move_dist,ncessch,year)) %>% head(2000) %>% View
```


### Looking at flags
c - single coordinates verification. If a school only has a single pair of coordinates for all years on google maps, it is unlikely that the address(es) provided are associated with more than one location. 
l - latlon verification. the distance between the google coordinates and manual coordinates (last batch) is less than 1000ft. The distance between the google coordinates and the geocoded coordinates is also less than 1000ft. 
r - radius confirmation. For each school, checks if the latitude-longitude of the googlemaps lookup remains within a 1000ft radius. or it uses. Confirms that the school has never moved based on clustering of the address data provided. Not tolerant of typos. These schools have their address, latitude and longitude changed to the most frequent value in years with a valid latitude. Applies to schools that only have a single location as well. 
R - the location moves continuously from one location to the next. 
z - zip code in dataset and google maps lookup are the same (note that these were not looked up using the zip code, just the street, city and state)

Verifications that are implemented. 

```{r}
# schools that have potential errors in the location
df[is.na(wildcard) & !grepl("l",location_verification_method) & ncessch %!in% unique(dv$ncessch)] %>% select(ncessch,year,school_name,street_location,city_location,zip_location,gm_formatted_address,gm_lat,gm_lon,enrollment,ccd_lat,ccd_lon)

# all schools in the manually sorted sections
df[ncessch %in% unique(dv$ncessch)] %>% arrange(desc(max_move_dist)) %>% select(max_move_dist,ncessch,year,gm_formatted_address,street_location,city_location,zip_location,gm_lat,gm_lon,school_name,wildcard,location_verification_method, enrollment,charter,ccd_lat,ccd_lon, everything()) %>% head(2000) %>% View

# all schools not in the manually sorted sections
df[ncessch %!in% unique(dv$ncessch)] %>% arrange(desc(max_move_dist)) %>% select(max_move_dist,ncessch,year,gm_formatted_address,street_location,city_location,zip_location,gm_lat,gm_lon,school_name,wildcard,location_verification_method, enrollment,charter,ccd_lat,ccd_lon, everything()) %>% arrange(desc(max_move_dist,ncessch,year)) %>% head(2000) %>% View

# look up single ncessch
# look up single ncessch
df[ncessch == 120150001477]%>% select(max_move_dist,ncessch,year,gm_formatted_address,street_location,city_location,zip_location,gm_lat,gm_lon,school_name,wildcard,location_verification_method, enrollment,charter,ccd_lat,ccd_lon, everything()) %>% head(2000) %>% View

# schools with missing lat or lon
df[is.na(gm_lat) & is.na(manual_lat_original)]

df[is.na(wildcard),location_verification_method] %>% table()
df[!is.na(wildcard),location_verification_method] %>% table()
df[,location_verification_method] %>% table()
```

Looking
```{r}
# look at the number of schools flagged by each verification method. DISTANCE IN MILES
setkey(df,ncessch,year)
df[,.(ncessch,year,school_name,gm_formatted_address,street_location,city_location,zip_location,gm_lon,gm_lat,geo_longitude,geo_latitude,manual_lon_original,manual_lat_original,
      gmvsgeo = geosphere::distHaversine(matrix(c(gm_lon,gm_lat),ncol=2,byrow=F), matrix(c(geo_longitude,geo_latitude),ncol=2,byrow=F))*0.000621371,
      gmvsll = geosphere::distHaversine(matrix(c(gm_lon,gm_lat),ncol=2,byrow=F), matrix(c(manual_lon_original,manual_lat_original),ncol=2,byrow=F))*0.000621371,
      maxdist = max(geosphere::distHaversine(matrix(c(median(gm_lon,na.rm=T),median(gm_lat,na.rm=T)),ncol=2,byrow=F), matrix(c(gm_lon,gm_lat),ncol=2,byrow=F)),na.rm=T)*0.000621371,
      zip_in_gm_address = stringi::stri_detect_fixed(gm_formatted_address, zip_location),
      zip_location_original, street_location_original, city_location_original, location_verification_method
      )] %>% 
  #filter(!zip_in_gm_address) %>% View
  group_by(location_verification_method) %>% summarize(n=n(), gmvsgeo = mean(gmvsgeo,na.rm=T), gmvsll = mean(gmvsll,na.rm=T), maxdist = max(geosphere::distHaversine(matrix(c(median(gm_lon,na.rm=T),median(gm_lat,na.rm=T)),ncol=2,byrow=F), matrix(c(gm_lon,gm_lat),ncol=2,byrow=F)),na.rm=T))

# number of schools / observations that have no verification for their address. 
df[location_verification_method == "",ncessch] %>% unique %>% length

# there are duplicates
df[,.(.N,vals=paste0(location_verification_method,collapse=", ")),by=.(ncessch,year)][N>1]

# hierarchical clustering
lids[,.(lat = median(gm_lat,na.rm=T), lon = median(gm_lon,na.rm=T)),by=cluster]
ggplot(lids,aes(x=gm_lon, y=gm_lat)) + geom_point()
plot(hc)
lids
geosphere::distHaversine(c(-80.06602,26.72873),lids[,.(gm_lon,gm_lat)])*3.28084

names(df)
```

Exloring
```{r}
setkey(df,ncessch,year)

# schools that are not latlon verified
df[ncessch %in% df[!stringi::stri_detect_fixed(location_verification_method, "l"),ncessch,],last(school_name),by=ncessch]#[,.(ncessch,year)]

# random sample of 30 that are radius verified
df[ncessch %in% (df[grepl("cz",location_verification_method) & !grepl("exclude",location_verification_method),ncessch]%>% sample(30))] %>% select(gm_formatted_address,gm_lat,gm_lon,street_location,city_location,zip_location,ncessch,year,school_name,location_verification_method,everything())

# schools that aren't latlon verified by year
df[!stringi::stri_detect_fixed(location_verification_method, "l"),.N,by=year] %>% arrange(desc(N))
df[!stringi::stri_detect_fixed(location_verification_method, "l") & year==2017] %>% select(gm_formatted_address,street_location,zip_location, city_location,location_verification_method,gm_lat,gm_lon,geo_latitude,geo_longitude,latitude,longitude) %>% filter(stringi::stri_detect_fixed(gm_formatted_address,zip_location)) %>% View
```

Looking
```{r eval=F}
# lat/lon data post merge, including distance
setkey(df,ncessch,year)
df[,.(ncessch,year,school_name,gm_formatted_address,street_location,city_location,zip_location,gm_lon,gm_lat,geo_longitude,geo_latitude,manual_lon_original,manual_lat_original,
      gmvsgeo = geosphere::distHaversine(matrix(c(gm_lon,gm_lat),ncol=2,byrow=F), matrix(c(geo_longitude,geo_latitude),ncol=2,byrow=F)),
      gmvsll = geosphere::distHaversine(matrix(c(gm_lon,gm_lat),ncol=2,byrow=F), matrix(c(manual_lon_original,manual_lat_original),ncol=2,byrow=F)),
      zip_in_gm_address = stringi::stri_detect_fixed(gm_formatted_address, zip_location),
      zip_location_original, street_location_original, city_location_original, location_verification_method
      )] %>% 
  filter(!zip_in_gm_address) %>% View
  #group_by(location_verification_method) %>% summarize(n=n(), gmvsgeo = mean(gmvsgeo,na.rm=T), gmvsll = mean(gmvsll,na.rm=T))

# all zips associated with each street_location
df[,.(nzips = length(unique(zip_location)),
      zipvals = paste0(unique(zip_location),collapse=", ")),by=c("street_location","city_location")] %>% arrange(desc(nzips)) %>% filter(nzips == 1)
```

```{r eval=F}
# LAT AND LONGq2h9 ----

# look at schools that have never had a location
df[,.(nmissing = sum(street_location %in% c("Missing/not reported","")), 
      ntotal = .N),by=ncessch]%>%arrange(desc(nmissing/ntotal))

# look at most popular addresses
df[,sum(!is.na(ncessch)),by=.(street_location,city_location)] %>% arrange(desc(V1))

# look at most popular street locations
df$street_location %>% table %>% sort(decreasing=T)

# is there a school that has never had an address and city associated
df[,.(nstreet = sum(!is.na(street_location)), ncity = sum(!is.na(city_location))),by=ncessch] %>% arrange(nstreet,ncity)
```

Looking
```{r}
## Lookup a school
{  t_num <- 120087008549
  df[ncessch == t_num,.(street_location,city_location,zip_location,ncessch,year,school_name,gm_formatted_address,gm_lat,gm_lon,median_lat,location_verification_method,enrollment,manual_lat_original,manual_lon_original,geo_latitude,geo_longitude,school_status,key_sch,virtual_original)] %>% arrange(year) #%>% DT::datatable(width=1400)
  #ui[ncessch == 120003000014	& year == 2009]
}

## lookup dist matrix for a school
{  t_num <- 120108002565
  df[ncessch == t_num & !is.na(street_location),stringdist::stringdistmatrix(.SD[,street_location],method="lcs")] -> d
  df[ncessch == t_num & !is.na(street_location)] %>% mutate(cluster = cutree(hclust(d), h=4)) %>%
    group_by(cluster) %>% 
    summarize(street_location = names(sort(table(street_location),decreasing=T))[1],n=n(), latitude = first(latitude), longitude = first(longitude))
}

## grep addresses for a string
df[ncessch %in% df[grepl("133270 HWY 90 W",street_location),ncessch],] %>% arrange(ncessch,year) %>% select(ncessch, year, school_name, street_location,city_location,zip_location, manual_lat_original,manual_lon_original,geo_latitude,geo_longitude,ccd_lat,ccd_lon,everything()) #%>% select(geo_latitude,geo_longitude) %>% unique()
```

More lookup
```{r eval=F}
# view closest matches to substring
df %>% mutate(kk = nchar(street_location) - stringdist::stringdist("",street_location)) %>% arrange(desc(kk)) %>% 
  select(ncessch,year,street_location,city_location,everything()) %>% filter(kk<6)

# view observations that are in the first query but not in the second one
t_str <- "PLACE"
df[grepl(t_str,street_location) & !grepl(paste0("\\b",t_str,"\\b"),street_location)] %>% 
  select(ncessch,year,street_location,city_location,everything())

# all substrings of addresses, sorted by frequency
df$street_location %>% strsplit(.," ") %>% unlist() %>% table(useNA = "always") %>% sort(decreasing = T) %>% head(80)

# unique pairs of coofrinates
df[,.(gm_lat,gm_lon)] %>% unique
df[,.(latitude,longitude)] %>% unique
df[,.(geo_latitude,geo_longitude)] %>% unique
```

```{r eval=F}
# look at string dist for each individual ncessch
df[!is.na(street_location),.(mdist = stringdist::stringdistmatrix(.SD[,street_location], method="lcs") %>% max(na.rm=T),
      numdist = stringdist::stringdistmatrix(.SD[,gsub("[^0-9d]","",street_location)],method="lcs") %>% max(na.rm=T)),by=ncessch] %>% arrange(mdist) %>% as.data.table() -> t_d

# look at number of locations associated with each street_location:city_location pair
df[!is.na(street_location),
   .(nlocations = nrow(unique(.SD[!is.na(geo_latitude) & !is.na(geo_longitude),.(geo_latitude,geo_longitude)]))),
   by=.(street_location,city_location)] %>% filter(nlocations > 1)#%>% pull(nlocations) %>% table(useNA="always") #%>% ggplot(.,aes(x=nlocations)) + geom_bar()

df[ncessch %in% t_d[mdist>4 & mdist<=7 & numdist==1,ncessch]] %>% select(ncessch, year, school_name,street_location, city_location,zip_location, virtual,k12_enrollment)

t_d[,.(mdist=first(mdist), numdist=first(numdist)),by=ncessch]

# school with distance
df[ncessch %in% t_d[numdist > 1, ncessch]] %>% select(street_location,city_location,zip_location,everything())

df[ncessch %in% df[,"Yes" %in% virtual_original & "No" %in% virtual_original,by=ncessch][V1==T,ncessch]] %>% select (ncessch, year, school_name,virtual_original, virtual,k12_enrollment) %>% fwrite("virtual.csv")

names(df)
```

Data Vaslidation

```{r}
# look at schools that did not pass zip validation
df[!grepl("z",location_verification_method)] %>% select(gm_formatted_address, zip_location,everything())

df[grepl("\\&",street_location)]%>% select(gm_formatted_address, zip_location,everything())

# arrange street location in order of increasing characters
df %>% arrange((nchar(street_location))) %>% select(street_location,city_location,ncessch,year,school_name,school_type,enrollment,latitude,longitude,geo_latitude,geo_longitude,school_status,key_sch,virtual_original)

# make sure that coordinates are mapped 1:1 with addresses
if (df[,.(nrow(unique(.SD[,.(gm_lat,gm_lat)]))),by=c("street_location","city_location")] %>% pull(V1) %>% max > 1) warning("Each street_location:city_location pair is not uniquely mapped to a coorinate pair. \n")
```



# End







































