---
title: "Charter Data Final"
output: html_notebook
---

## Prepare

Init session
```{r echo=F, warning=F, message=F}
rm(list = ls())
library(tidyverse) # imagine not typing library(tidyverse) at the top of an R notebook
library(data.table) # best package NA
library(stringdist) # fuzzy string matching (for address comparison)
library(geosphere) # distHaversine() between two latlon coordinate pairs
library(Matrix) # sparse matrix multiplication is sped up
library(plm)  # fixed effects regression
library(gtools) # combinations 
library(tictoc) # benchmarking
library(parallel) # faster computation
library(caret) # k-fold cross validation
# library(texreg) # display regression results better

options(scipen=34)
`%!in%` <- function(a,b) !a %in% b
s <- function() s_df <<- df
l <- function() df <<- s_df
commons <- function(k, n = 20, dec=T)  k %>% table(useNA="a") %>% sort(d=dec) %>% head(n)
df <- fread("sch_include_03_01_21.csv", integer64="numeric")
# mode <- function(a)  names(sort(table(a), decreasing=T))[1]
# rm(list = ls()[grep("t_.*",ls())])
```

Transform data sources and load into dataset
```{r eval=F}
tictoc::tic()
# Urban Institute ----
# ui
source("src/transform_urban_institute.R")
# it is intentional that NAs are introduced by coercion

# Gradewise Enrollment ----
# gwe
source("src/transform_gradewise_enr.R") 

# Old Dataset ----
df_old <- fread("../Research Question 1/sch_complete_05_26_20.csv", integer64 = "numeric") %>% as.data.table
if ("V1" %in% names(df_old)) { df_old[,V1:=NULL] }

# Create main df
source("src/load_df.R")

# Extract and transform location data
source("src/transform_location.R") 
# s()
# 61586 rows
l()
# Extract and transform school grades & testing data
source("src/transform_grades.R")

rm(df_old,gwe,ui,sg,t_) # Remove source data from workspace
tictoc::toc()

# EXPORT TO CSV ----
# fwrite(df[manual_exclude==F],"sch_include_03_01_21.csv")
# fwrite(dm,"distance_matrix.csv")
# fwrite(ui[!df],"sch_exclude_07_08_20.csv")
# fwrite(df[manual_exclude==T],"sch_exclude_addl_07_08_20.csv")
```

Generate charter student presence measure
```{r}
# 244.114/196.847 seconds with sparse matrix
# 375.247 seconds without sparse matrix

tictoc::tic()

# INIT HYPPERPARAMETERS ----
# max distance where that schools are accounted for, after this their weight is 0
dmax_set = c(20,40,60,80,100)

# a corresponds to the curve shape. 1 is straight line from (0,1) to (d,0), a > 1 is concave, a < 1 is convex. 
a_set = c(0.05,0.25,1,5,25)


# INIT OTHER ----


    # load add CSP to a previous previous dataset
    df <- fread("sch_include_02_18_21.csv", integer64="numeric")
    # OR 
    # create new measure of CSP in the current  data.table
    #df[,df %>% names %>% grep("csp",.) := NULL]


dm <- fread("src/distance_matrix.csv",integer64 = "double") %>% as.matrix()
# all grades to iterate through and average
grades <- paste0(c("k",paste0("grade",1:12)),"_enr")

y_enr = list(); y_cenr = list()
for (y in 1999:2018) {
    # create a list of enrollments by year. row is location ID, column is grade
    y_enr[[y]] <- (merge(data.table(location_id=1:nrow(dm)),df[year == y, lapply(.SD,function(x) sum(x,na.rm=T)), by=location_id, .SDcols = grades], by="location_id", all.x=T) %>% select(2:14) %>% replace(.,is.na(.),0) %>% as.matrix)
    # & create a list of charter enrollments by year. row is location ID, column is grade
    y_cenr[[y]] <- (merge(data.table(location_id=1:nrow(dm)),df[year == y & charter=="yes", lapply(.SD,function(x) sum(x,na.rm=T)), by=location_id, .SDcols = grades], by="location_id", all.x=T) %>% select(2:14) %>% replace(.,is.na(.),0) %>% as.matrix)
}


# LOOP ----
# iterate through all values of max distance
for (dmax in dmax_set) { 
  # where distance >= distance max, set the weight to 0
  dmf <- (dm/dmax); dmf[dmf > 1] <- 1
  
  # iterate through all curve-shaping values
  for (a in a_set) {
    paste0("\na=",a,"; dmax=",dmax) %>% cat
    # apply the curving function to the distance matrix
    dmf <- Matrix(1-(dmf^a),sparse=T)
        
    # iterate through all years
    for (y in 1999:2018) {
      
      # STUDENT PRESENCE proximate to location I, weighted by distance curving function
      sp <- data.table(as.matrix(dmf %*% y_enr[[y]])); setnames(sp,paste0('sp_',grades))
      # CHARTER STUDENT PRESENCE proximate to location I, weighted by distance curving function
      csp <- data.table(as.matrix(dmf %*% y_cenr[[y]])); setnames(csp,paste0('csp_',grades))
      # add location_id and year columns so that a join with df is possible
      # if this throws an error it means your dm and 
      sp[,`:=`(location_id = 1:nrow(dm), year=y)]; csp[,`:=`(location_id = 1:nrow(dm), year=y)]
      
      # data.table of location ids and 
      if (y==1999) t_lids <- merge(sp,csp,on=.(location_id,year))
      else t_lids <- rbind(t_lids, merge(sp,csp,on=.(location_id,year)))
    }
  
    # generate a column representing charter student presence given dmax and a
    t_m <- merge(select(df,location_id,year,ncessch,k12_enrollment,grades),t_lids,by=c("location_id","year"))
    t_m[[paste0("csp","_a",a,"_dmax",dmax)]] <- (rowSums((as.matrix(select(t_m, paste0("csp_",grades))) / as.matrix(select(t_m, paste0("sp_",grades,""))))  *  as.matrix(select(t_m,grades))) / as.matrix(t_m$k12_enrollment))
    df <- merge(df,select(t_m,year,ncessch,paste0("csp","_a",a,"_dmax",dmax)),by=c("ncessch","year"))
  }
}
# suppressWarnings(rm(y_cenr,y_enr,t_lids,sp,csp,a,dmax,dmf))
tictoc::toc()
```

## Regressions & Analysis

Delete anything in here:
```{r}
t_func <- function(i) {
  2*i
  
}

tic(); mclapply(1:nrow(g), t_func); toc()
```

Model that will estimate runtime of program
```{r}
tt_ = data.table(nrows = c(1000,2000,3600,8000,12000,5341,14000,16000,20000,53468),time = c(0.216,0.423,1.366,5.899,16.415,2.436,23.395,33.836,60.7,868.486))
tm_ <- lm(time ~ nrows + I(nrows^2),tt_)
tm_ %>% summary()
predict(tm_, newdata=data.frame(nrows = 53468))
    # with 20,000 rows the model predicts 55 seconds
    # with 53,468 observations the model predicts 533.2522 seconds. I predict it will take 600 seconds. It actually took 868
    # 868 * 525 / 3  is the total runtime with parallelization, but no crossfold validation. 41 hours, could be much longer. 20 hours with 10fold, using all 6 lags. LOOCV seems out of the question.  
ggplot(tt_) + geom_point(aes(x=nrows,y=time))
```

Fixed effects regression in R
```{r}
# init
dmax_set = c(20,40,60,80,100); a_set = c(0.05,0.25,1,5,25); depvar_set = "combined_proficiency"; 
lags_set = combinations(7,2,c(0:6)); lags_set = paste(lags_set[,1],lags_set[,2],sep=":"); lags_set = "0:6"
n_folds = 10


# create panel data frame 
dfp <- pdata.frame(df[charter=="no"],index=c("ncessch","year")) %>% head(500)
dfp$combined_proficiency <- (dfp$comb_math_proficiency + dfp$comb_ela_proficiency)/2


# init gridsearch and plm functionn
g <- expand.grid(dmax = dmax_set,a = a_set, depvar = depvar_set, lags = lags_set, stringsAsFactors=F)
rr <- function(i) {
  r <- plm(as.formula(paste(g[i,]$depvar," ~ year + as.numeric(year) * as.factor(ncessch) + lag(csp_a",g[i,]$a,"_dmax",g[i,]$dmax,",",g[i,]$lags,")",sep="")), data=dfp, model="within")
  data.frame(g[i,]$dmax,g[i,]$a,g[i,]$lags,r.squared(r),g[i,]$depvar)
}


# run regression on each row of grid
tic(); regression_results <- mclapply(1:nrow(g), rr) %>% bind_rows; toc()
# rename the results
regression_results %>% setnames(.,c("max_distance","alpha","lags","rsquared","depvar"))

# 
# r <- plm(as.formula(paste(g[i,]$depvar," ~ year + as.numeric(year) * as.factor(ncessch) + lag(csp_a",g[i,]$a,"_dmax",g[i,]$dmax,",",g[i,]$lags,")",sep="")), data=dfp[-fold[[i]],], model="within")
# 
# 
# # DEBUGG THISS
# r.squared(r,dfp[fold[[2]],])
# 
# # ____  READ SOURCE CODE LUL ____
# r.squared
# methods("fitted")
#   getAnywhere(fitted.plm)
#     getAnywhere(model.matrix.plm)
#     getAnywhere(model.matrix)
#       getAnywhere(model.frame.pdata.frame)
#     
# stats:::model.frame
# methods(model.frame)
#     
# getAnywhere(describe)
# 
# 
# is.null(r$model)
# 
# fold <- createFolds(1:nrow(dfp),n_folds)
# for (i in 1:n_folds) {
#   r <- plm(as.formula(paste(g[i,]$depvar," ~ year + as.numeric(year) * as.factor(ncessch) + lag(csp_a",g[i,]$a,"_dmax",g[i,]$dmax,",",g[i,]$lags,")",sep="")), data=dfp[-fold[[i]],], model="within")
#   r.squared(r,)
#   summary(r)
# }
# 
# predict(r,dfp[fold[[1]],])
# 
# myrsquared(r,dfp[fold[[1]],])
```

```{r}
# myrsquared <- function (object, model = NULL, type = c("cor", "rss", "ess"), 
#     dfcor = FALSE) 
# {
#     if (is.null(model)) 
#         model <- describe(object, "model")
#     effect <- describe(object, "effect")
#     type <- match.arg(type)
#     if (type == "cor") {
#       print(1)
#         y <- pmodel.response(object, model = model, effect = effect)
#         print(2)
#         haty <- fitted(object, model = model, effect = effect)
#         print(3)
#         R2 <- cor(y, haty)^2
#     }
#     if (type == "rss") {
#         R2 <- 1 - deviance(object, model = model)/tss(object, 
#             model = model)
#     }
#     if (type == "ess") {
#         haty <- fitted(object, model = model)
#         mhaty <- mean(haty)
#         ess <- as.numeric(crossprod((haty - mhaty)))
#         R2 <- ess/tss(object, model = model)
#     }
#     if (dfcor) 
#         R2 <- 1 - (1 - R2) * (length(resid(object)) - 1)/df.residual(object)
#     R2
# }
# environment(myrsquared) <- 
```

```{r}
predict(r,dfp[fold[[1]],])
```

```{r}
# base regression, equivalent to:
# xtreg d.comb_math_proficiency l(1/6)d.comb_math_proficiency l(0/6)d.csp_a1_dmax20 i.year, fe
r <- plm(diff(comb_math_proficiency)~lag(diff(comb_math_proficiency), 1:6) + lag(diff(csp_a1_dmax20),0:6) + year, data=dfp, model="within"); summary(r)

# test FE regression with year effects
f <- as.formula(paste("comb_math_proficiency~","csp_a1_dmax20 + lag(csp_a1_dmax20) + year"))
plm(f,data=dfp,model="within")


dfp %>% filter(ncessch == 120003000002) %>% select(math, teacher_student_ratio)
```

----------

Visualize a set of location_id on map
```{r}
# library(ggplot2)
lid = sample(df$location_id,1); rad = 5; y = 1999; maprad=20; lid = 1
radset = (which(dm[lid,]<rad) %>% unname)
mapradset = (which(dm[lid,]<maprad) %>% unname)


# plot it 
ggplot(df[location_id %in% mapradset]) + 
  coord_fixed() + 
  geom_point(aes(x=gm_lon,y=gm_lat,color=location_id%in%radset),size=0.5) + 
  geom_point(data = df[location_id == lid],aes(x=gm_lon,y=gm_lat),size = 0.2, color = "black")

# density plot
y=2018
ggplot(df[location_id %in% radset & year == y],aes(x=gm_lon,y=gm_lat)) + 
  coord_fixed() + 
  #geom_point(aes(x=gm_lon,y=gm_lat,alpha=location_id%in%radset,color=charter,size=0.05)) + 
  geom_text(aes(label=k_enr+grade1_enr+grade2_enr+grade3_enr+grade4_enr+grade5_enr,color=charter))+
  geom_point(data = df[location_id == lid],aes(x=gm_lon,y=gm_lat),size = 0.2, color = "black")

# sum of charter & tps enrollment in the given radius and year
q1 <- df[location_id %in% radset,.(
                             allschools = sum(k_enr+grade1_enr+grade2_enr+grade3_enr+grade4_enr+grade5_enr),
                             charterschools=sum((charter=="yes")*(k_enr+grade1_enr+grade2_enr+grade3_enr+grade4_enr+grade5_enr))
                             ),by=year] %>% mutate(ratio=charterschools/allschools)

# charter penetration
q2 <- df[location_id == lid] %>% select(location_id,year,csp_a1_dmax20, csp_a0.25_dmax20, csp_a0.05_dmax20, csp_a5_dmax20, csp_a25_dmax20)

merge(q1,q2,on=year) %>% view

# view all rows in year with lid
df[year == y & location_id %in% radset] %>% select(grades,everything()) %>% view
# view results of multiplication at LID
(dmf %*% y_enr[[y]])[lid,]
```

Manuel FE Regression
```{r}
df[order(year),.(year,comb_math_proficiency,
                 d.FE.comb_math_proficiency=c(NA,diff(comb_math_proficiency))-mean(diff(comb_math_proficiency))
                 ),by=.(ncessch)][!is.na(comb_math_proficiency)]

# create differenced outcome measures subtracting the mean as a proxy for FE regression
df[order(year),`:=`(
  d.FE.comb_math_proficiency=(c(NA,diff(comb_math_proficiency))-mean(diff(comb_math_proficiency))),
  d.FE.comb_ela_proficiency=(c(NA,diff(comb_ela_proficiency))-mean(diff(comb_ela_proficiency))),
  d.FE.comb_avg_proficiency=(c(NA,diff(comb_ela_proficiency+comb_math_proficiency)/2)-mean(diff(comb_ela_proficiency+comb_math_proficiency)/2))
  ),by=.(ncessch)]

# create lags of these differences, and of charter competition
df[order(year), `:=`(
  l1d.FE.comb_math_proficiency=shift(c(NA,diff(comb_math_proficiency)-mean(diff(comb_math_proficiency))),1),
  l2d.FE.comb_math_proficiency=shift(c(NA,diff(comb_math_proficiency)-mean(diff(comb_math_proficiency))),2),
  l3d.FE.comb_math_proficiency=shift(c(NA,diff(comb_math_proficiency)-mean(diff(comb_math_proficiency))),3),
  l4d.FE.comb_math_proficiency=shift(c(NA,diff(comb_math_proficiency)-mean(diff(comb_math_proficiency))),4),
  l5d.FE.comb_math_proficiency=shift(c(NA,diff(comb_math_proficiency)-mean(diff(comb_math_proficiency))),5),
  l6d.FE.comb_math_proficiency=shift(c(NA,diff(comb_math_proficiency)-mean(diff(comb_math_proficiency))),6),
  
)]

df[,.()]
df[order(year),.(comb_math_proficiency, shift(comb_math_proficiency,1),l1d.FE.comb_math_proficiency),by=.(ncessch)]

lm(data=df,
  d.FE.comb_avg_proficiency~lag(d.FE.comb_avg_proficiency))

df[,.(ncessch, year, d.FE.comb_avg_proficiency, lag(d.FE.comb_avg_proficiency,0:2))]
```

Playing with plm
```{r}
r <- plm(diff(comb_math_proficiency,0)~diff(comb_math_proficiency,c(2:7)) + diff(csp_a1_dmax20,c(0:6)) + year,
         data=dfp, model="within"); r

diff(dfp$comb_math_proficiency,lag=c(2:6))

data.frame("first difference"=diff(dfp$comb_math_proficiency),"thevalue"=dfp$comb_math_proficiency,diff(dfp$comb_math_proficiency,lag=c(1:6)))
```

## Export

```{r}
#        s()          l()

# make sure all names will play nice in stata

# k_enr -> grade0_enr
# setnames(df, names(df)[grepl("exp_decay_k_enr",names(df))], names(df)[grepl("exp_decay_k_enr",names(df))] %>% gsub("k_enr","grade0_enr",.))

# abbreviate pct
setnames(df, names(df), gsub("percente?","pct" ,names(df)))

# replace names
setnames(df, c("english_language_arts_gains_of_lowest_25%", "college_&_career_acceleration(prev_year)", "pct_of_economically_disadvantaged_students", "pct_level_3_and_above_fcat_reading" , "pct_making_learning_gains_in_reading",  "pct_making_learning_gains_in_math", "pct_of_lowest_25p_making_learning_gains_in_reading","k_enr"), 
         c("ela_gains_lowest_25","college_and_career_acc", "pct_econ_disadv_students", "pct_l3_and_above_reading","pct_making_gains_reading",  "pct_making_gains_math", "pct_lowest_25p_gains_reading","grade0_enr"), skip_absent = T)

# drop fully irrelevant columns
(df[,c("gm_street_number","gm_route","gm_postal_code"):=NULL])

# order the columns the way that I like
df %>% select(
  ncessch,year,
  school_name, distinct_charter, distinct_regular_nc,
  k12_enrollment, 
  gm_formatted_address, gm_lat,gm_lon,
  street_location, city_location, zip_location,
  school_type,  charter, 
  c(names(df)[grepl(".*_enr",names(df))]),
  location_verification_method, latlon_verification_method,
  everything()
) -> df

# fwrite(ui[!df],"sch_exclude_07_08_20.csv")
# fwrite(df[manual_exclude==T],"sch_exclude_addl_07_08_20.csv")
fwrite(df[manual_exclude==F],"sch_include_02_17_21.csv")

# validation ----


if (!identical(as.character(), names(df)[grepl("\\.[xy]",names(df))])) warning("There are columns with a .x or .y suffix")
if (!identical(as.character(), names(df)[nchar(names(df)) >= 32])) warning("There are columns with atleast 32 characters")
```


## Data Sources

***Data Sources***

All sources wisll eventually contain data 1999 through 2018, some are currently a year or two behind 2018

*Urban Institute Dataset*
(https://educationdata.urban.org/data-explorer/schools/) downloaded 06/04/20 
Most information comes from the common core of data [partial data dictionary](https://nces.ed.gov/ccd/psadd.asp)
contains general information about:
- enrollment
- lowest/highest grade offered
- charter status
- virtual status
- latitude & longitude
```{r eval=F}
ui %>% sample_n(20)
```

*Latitude & Longitude*
Pulled from one file from ELSI, source of coordinates is CCD
(https://nces.ed.gov/ccd/elsi/) downloaded 06/15/20

*Grade-wise enrollment* 
Pulled as four separate files from ELSI table generator, then combined 
(https://nces.ed.gov/ccd/elsi/) downloaded 06/05/20
```{r eval=F}
gwe %>% select(ncessch, year, everything()) %>% sample_n(10)
```

*School grades datasets*
(http://www.fldoe.org/accountability/accountability-reporting/school-grades/archives.stml)
1999 - 2017
- Test scores
- School grade
```{r eval=F}
sg2014 %>%
  select(key_sch, distnum_schnum,`District Number`,`School Number`,Year,everything()) %>% sample_n(10)

sg2017 %>%
  select(key_sch, distnum_schnum,`District Number`,`School Number`,Year,everything()) %>% sample_n(10)
```

*Student demographics*
(https://nces.ed.gov/ccd/elsi/tableGenerator.aspx)
1997 - 2016
- Total enrollment
- Highest and lowest grade
- Teachers & pupil teacher ratio
```{r eval=F}
dem %>% sample_n(20)
dem$Year %>% table
```

### Misc. notes
1609.34 meters in a mile.

## Looking: 
### Names
Stringdist matrix
```{r}
df[!is.na(street_location) & ncessch %!in% unique(dv$ncessch),.(mdist = stringdist::stringdistmatrix(.SD[,street_location], method="lcs") %>% max(na.rm=T),
      numdist = stringdist::stringdistmatrix(.SD[,gsub("[^0-9d]","",street_location)],method="lcs") %>% max(na.rm=T)),by=ncessch] %>% arrange(mdist) %>% as.data.table() -> t_d

t_d[mdist != -Inf] %>% arrange(desc(mdist + numdist*4))

t_d[mdist != -Inf & (mdist + 4*numdist)<20,ncessch] -> t_d2
df[ncessch %!in% unique(dv$ncessch) & ncessch %in% t_d2] %>% arrange(desc(max_move_dist)) %>% select(max_move_dist,ncessch,year,gm_formatted_address,street_location,city_location,zip_location,gm_lat,gm_lon,school_name,wildcard,location_verification_method, enrollment,charter,ccd_lat,ccd_lon, everything()) %>% arrange(desc(max_move_dist,ncessch,year)) %>% head(2000) %>% View

# unique names of schools that have been open since 2014

df$school_name %>% unique %>% length


str_split(t_names$name,"") %>% unlist %>% table()

#rm(loc_enr,loc_enr2,dm_bool,y,charter_ids,tps_ids,big_enr_charter,big_enr_tps)
```

Googlemaps unique name queries
```{r}
t_names <- fread("src/gm_addresses3.csv") %>% as.data.table(key="name_original")
t_names_merge <- merge(df,t_names,by.x="school_name",by.y="name_original",all.x=T)
t_names_merge[year == 2018 & !is.na(gm_lon.y) & !is.na(gm_lat.y),.(dist = geosphere::distHaversine(c(first(gm_lon.x),first(gm_lat.x)),c(first(gm_lon.y),first(gm_lat.y))), nces = paste0(unique(ncessch),collapse=", ")),by=school_name] %>% arrange(desc(dist))
#t_names_merge[year == 2018 & !is.na(gm_lon.y) & !is.na(gm_lat.y),.(dist = geosphere::distHaversine(c(first(gm_lon.x),first(gm_lat.x)),c(first(gm_lon.y),first(gm_lat.y))))]

t_names_merge[.(paste0(c(gm_lon.x,gm_lat.x),c(gm_lon.y,gm_lat.y)))]
```

```{r}
t_names_merge[ncessch == 120198002022] %>% select(ncessch, year, school_name, gm_lat.x,gm_lon.x,gm_formatted_address,gm_lat.y,gm_lon.y,gm_address)
```

```{r}
t_names_merge[ncessch %in% 120198002022] %>% arrange(desc(max_move_dist)) %>% select(max_move_dist,ncessch,year,gm_formatted_address,street_location,city_location,zip_location,gm_lon.x,gm_lat.x,gm_lon.y,gm_lat.y,school_name,wildcard,location_verification_method, enrollment,charter,ccd_lat,ccd_lon, everything()) %>% arrange(desc(max_move_dist,ncessch,year)) %>% head(2000) %>% View
```


### Looking at flags
c - single coordinates verification. If a school only has a single pair of coordinates for all years on google maps, it is unlikely that the address(es) provided are associated with more than one location. 
l - latlon verification. the distance between the google coordinates and manual coordinates (last batch) is less than 1000ft. The distance between the google coordinates and the geocoded coordinates is also less than 1000ft. 
r - radius confirmation. For each school, checks if the latitude-longitude of the googlemaps lookup remains within a 1000ft radius. or it uses. Confirms that the school has never moved based on clustering of the address data provided. Not tolerant of typos. These schools have their address, latitude and longitude changed to the most frequent value in years with a valid latitude. Applies to schools that only have a single location as well. 
R - the location moves continuously from one location to the next. 
z - zip code in dataset and google maps lookup are the same (note that these were not looked up using the zip code, just the street, city and state)

Verifications that are implemented. 

```{r}
# schools that have potential errors in the location
df[is.na(wildcard) & !grepl("l",location_verification_method) & ncessch %!in% unique(dv$ncessch)] %>% select(ncessch,year,school_name,street_location,city_location,zip_location,gm_formatted_address,gm_lat,gm_lon,enrollment,ccd_lat,ccd_lon)

# all schools in the manually sorted sections
df[ncessch %in% unique(dv$ncessch)] %>% arrange(desc(max_move_dist)) %>% select(max_move_dist,ncessch,year,gm_formatted_address,street_location,city_location,zip_location,gm_lat,gm_lon,school_name,wildcard,location_verification_method, enrollment,charter,ccd_lat,ccd_lon, everything()) %>% head(2000) %>% View

# all schools not in the manually sorted sections
df[ncessch %!in% unique(dv$ncessch)] %>% arrange(desc(max_move_dist)) %>% select(max_move_dist,ncessch,year,gm_formatted_address,street_location,city_location,zip_location,gm_lat,gm_lon,school_name,wildcard,location_verification_method, enrollment,charter,ccd_lat,ccd_lon, everything()) %>% arrange(desc(max_move_dist,ncessch,year)) %>% head(2000) %>% View

# look up single ncessch
# look up single ncessch
df[ncessch == 120150001477]%>% select(max_move_dist,ncessch,year,gm_formatted_address,street_location,city_location,zip_location,gm_lat,gm_lon,school_name,wildcard,location_verification_method, enrollment,charter,ccd_lat,ccd_lon, everything()) %>% head(2000) %>% View

# schools with missing lat or lon
df[is.na(gm_lat) & is.na(manual_lat_original)]

df[is.na(wildcard),location_verification_method] %>% table()
df[!is.na(wildcard),location_verification_method] %>% table()
df[,location_verification_method] %>% table()
```

Looking
```{r}
# look at the number of schools flagged by each verification method. DISTANCE IN MILES
setkey(df,ncessch,year)
df[,.(ncessch,year,school_name,gm_formatted_address,street_location,city_location,zip_location,gm_lon,gm_lat,geo_longitude,geo_latitude,manual_lon_original,manual_lat_original,
      gmvsgeo = geosphere::distHaversine(matrix(c(gm_lon,gm_lat),ncol=2,byrow=F), matrix(c(geo_longitude,geo_latitude),ncol=2,byrow=F))*0.000621371,
      gmvsll = geosphere::distHaversine(matrix(c(gm_lon,gm_lat),ncol=2,byrow=F), matrix(c(manual_lon_original,manual_lat_original),ncol=2,byrow=F))*0.000621371,
      maxdist = max(geosphere::distHaversine(matrix(c(median(gm_lon,na.rm=T),median(gm_lat,na.rm=T)),ncol=2,byrow=F), matrix(c(gm_lon,gm_lat),ncol=2,byrow=F)),na.rm=T)*0.000621371,
      zip_in_gm_address = stringi::stri_detect_fixed(gm_formatted_address, zip_location),
      zip_location_original, street_location_original, city_location_original, location_verification_method
      )] %>% 
  #filter(!zip_in_gm_address) %>% View
  group_by(location_verification_method) %>% summarize(n=n(), gmvsgeo = mean(gmvsgeo,na.rm=T), gmvsll = mean(gmvsll,na.rm=T), maxdist = max(geosphere::distHaversine(matrix(c(median(gm_lon,na.rm=T),median(gm_lat,na.rm=T)),ncol=2,byrow=F), matrix(c(gm_lon,gm_lat),ncol=2,byrow=F)),na.rm=T))

# number of schools / observations that have no verification for their address. 
df[location_verification_method == "",ncessch] %>% unique %>% length

# there are duplicates
df[,.(.N,vals=paste0(location_verification_method,collapse=", ")),by=.(ncessch,year)][N>1]

# hierarchical clustering
lids[,.(lat = median(gm_lat,na.rm=T), lon = median(gm_lon,na.rm=T)),by=cluster]
ggplot(lids,aes(x=gm_lon, y=gm_lat)) + geom_point()
plot(hc)
lids
geosphere::distHaversine(c(-80.06602,26.72873),lids[,.(gm_lon,gm_lat)])*3.28084

names(df)
```

Exloring
```{r}
setkey(df,ncessch,year)

# schools that are not latlon verified
df[ncessch %in% df[!stringi::stri_detect_fixed(location_verification_method, "l"),ncessch,],last(school_name),by=ncessch]#[,.(ncessch,year)]

# random sample of 30 that are radius verified
df[ncessch %in% (df[grepl("cz",location_verification_method) & !grepl("exclude",location_verification_method),ncessch]%>% sample(30))] %>% select(gm_formatted_address,gm_lat,gm_lon,street_location,city_location,zip_location,ncessch,year,school_name,location_verification_method,everything())

# schools that aren't latlon verified by year
df[!stringi::stri_detect_fixed(location_verification_method, "l"),.N,by=year] %>% arrange(desc(N))
df[!stringi::stri_detect_fixed(location_verification_method, "l") & year==2017] %>% select(gm_formatted_address,street_location,zip_location, city_location,location_verification_method,gm_lat,gm_lon,geo_latitude,geo_longitude,latitude,longitude) %>% filter(stringi::stri_detect_fixed(gm_formatted_address,zip_location)) %>% View
```

Looking
```{r eval=F}
# lat/lon data post merge, including distance
setkey(df,ncessch,year)
df[,.(ncessch,year,school_name,gm_formatted_address,street_location,city_location,zip_location,gm_lon,gm_lat,geo_longitude,geo_latitude,manual_lon_original,manual_lat_original,
      gmvsgeo = geosphere::distHaversine(matrix(c(gm_lon,gm_lat),ncol=2,byrow=F), matrix(c(geo_longitude,geo_latitude),ncol=2,byrow=F)),
      gmvsll = geosphere::distHaversine(matrix(c(gm_lon,gm_lat),ncol=2,byrow=F), matrix(c(manual_lon_original,manual_lat_original),ncol=2,byrow=F)),
      zip_in_gm_address = stringi::stri_detect_fixed(gm_formatted_address, zip_location),
      zip_location_original, street_location_original, city_location_original, location_verification_method
      )] %>% 
  filter(!zip_in_gm_address) %>% View
  #group_by(location_verification_method) %>% summarize(n=n(), gmvsgeo = mean(gmvsgeo,na.rm=T), gmvsll = mean(gmvsll,na.rm=T))

# all zips associated with each street_location
df[,.(nzips = length(unique(zip_location)),
      zipvals = paste0(unique(zip_location),collapse=", ")),by=c("street_location","city_location")] %>% arrange(desc(nzips)) %>% filter(nzips == 1)
```

```{r eval=F}
# LAT AND LONGq2h9 ----

# look at schools that have never had a location
df[,.(nmissing = sum(street_location %in% c("Missing/not reported","")), 
      ntotal = .N),by=ncessch]%>%arrange(desc(nmissing/ntotal))

# look at most popular addresses
df[,sum(!is.na(ncessch)),by=.(street_location,city_location)] %>% arrange(desc(V1))

# look at most popular street locations
df$street_location %>% table %>% sort(decreasing=T)

# is there a school that has never had an address and city associated
df[,.(nstreet = sum(!is.na(street_location)), ncity = sum(!is.na(city_location))),by=ncessch] %>% arrange(nstreet,ncity)
```

Looking
```{r}
## Lookup a school
{  t_num <- 120087008549
  df[ncessch == t_num,.(street_location,city_location,zip_location,ncessch,year,school_name,gm_formatted_address,gm_lat,gm_lon,median_lat,location_verification_method,enrollment,manual_lat_original,manual_lon_original,geo_latitude,geo_longitude,school_status,key_sch,virtual_original)] %>% arrange(year) #%>% DT::datatable(width=1400)
  #ui[ncessch == 120003000014	& year == 2009]
}

## lookup dist matrix for a school
{  t_num <- 120108002565
  df[ncessch == t_num & !is.na(street_location),stringdist::stringdistmatrix(.SD[,street_location],method="lcs")] -> d
  df[ncessch == t_num & !is.na(street_location)] %>% mutate(cluster = cutree(hclust(d), h=4)) %>%
    group_by(cluster) %>% 
    summarize(street_location = names(sort(table(street_location),decreasing=T))[1],n=n(), latitude = first(latitude), longitude = first(longitude))
}

## grep addresses for a string
df[ncessch %in% df[grepl("133270 HWY 90 W",street_location),ncessch],] %>% arrange(ncessch,year) %>% select(ncessch, year, school_name, street_location,city_location,zip_location, manual_lat_original,manual_lon_original,geo_latitude,geo_longitude,ccd_lat,ccd_lon,everything()) #%>% select(geo_latitude,geo_longitude) %>% unique()
```

More lookup
```{r eval=F}
# view closest matches to substring
df %>% mutate(kk = nchar(street_location) - stringdist::stringdist("",street_location)) %>% arrange(desc(kk)) %>% 
  select(ncessch,year,street_location,city_location,everything()) %>% filter(kk<6)

# view observations that are in the first query but not in the second one
t_str <- "PLACE"
df[grepl(t_str,street_location) & !grepl(paste0("\\b",t_str,"\\b"),street_location)] %>% 
  select(ncessch,year,street_location,city_location,everything())

# all substrings of addresses, sorted by frequency
df$street_location %>% strsplit(.," ") %>% unlist() %>% table(useNA = "always") %>% sort(decreasing = T) %>% head(80)

# unique pairs of coofrinates
df[,.(gm_lat,gm_lon)] %>% unique
df[,.(latitude,longitude)] %>% unique
df[,.(geo_latitude,geo_longitude)] %>% unique
```

```{r eval=F}
# look at string dist for each individual ncessch
df[!is.na(street_location),.(mdist = stringdist::stringdistmatrix(.SD[,street_location], method="lcs") %>% max(na.rm=T),
      numdist = stringdist::stringdistmatrix(.SD[,gsub("[^0-9d]","",street_location)],method="lcs") %>% max(na.rm=T)),by=ncessch] %>% arrange(mdist) %>% as.data.table() -> t_d

# look at number of locations associated with each street_location:city_location pair
df[!is.na(street_location),
   .(nlocations = nrow(unique(.SD[!is.na(geo_latitude) & !is.na(geo_longitude),.(geo_latitude,geo_longitude)]))),
   by=.(street_location,city_location)] %>% filter(nlocations > 1)#%>% pull(nlocations) %>% table(useNA="always") #%>% ggplot(.,aes(x=nlocations)) + geom_bar()

df[ncessch %in% t_d[mdist>4 & mdist<=7 & numdist==1,ncessch]] %>% select(ncessch, year, school_name,street_location, city_location,zip_location, virtual,k12_enrollment)

t_d[,.(mdist=first(mdist), numdist=first(numdist)),by=ncessch]

# school with distance
df[ncessch %in% t_d[numdist > 1, ncessch]] %>% select(street_location,city_location,zip_location,everything())

df[ncessch %in% df[,"Yes" %in% virtual_original & "No" %in% virtual_original,by=ncessch][V1==T,ncessch]] %>% select (ncessch, year, school_name,virtual_original, virtual,k12_enrollment) %>% fwrite("virtual.csv")

names(df)
```

Data Vaslidation

```{r}
# look at schools that did not pass zip validation
df[!grepl("z",location_verification_method)] %>% select(gm_formatted_address, zip_location,everything())

df[grepl("\\&",street_location)]%>% select(gm_formatted_address, zip_location,everything())

# arrange street location in order of increasing characters
df %>% arrange((nchar(street_location))) %>% select(street_location,city_location,ncessch,year,school_name,school_type,enrollment,latitude,longitude,geo_latitude,geo_longitude,school_status,key_sch,virtual_original)

# make sure that coordinates are mapped 1:1 with addresses
if (df[,.(nrow(unique(.SD[,.(gm_lat,gm_lat)]))),by=c("street_location","city_location")] %>% pull(V1) %>% max > 1) warning("Each street_location:city_location pair is not uniquely mapped to a coorinate pair. \n")
```



# End



























